<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>dunyaoguz.github.io - Dunya Oguz</title><link href="https://dunyaoguz.github.io/my-blog/" rel="alternate"></link><link href="https://dunyaoguz.github.io/my-blog/feeds/dunya-oguz.atom.xml" rel="self"></link><id>https://dunyaoguz.github.io/my-blog/</id><updated>2019-05-05T17:00:00-04:00</updated><entry><title>Clustering Algorithms</title><link href="https://dunyaoguz.github.io/my-blog/clustering.html" rel="alternate"></link><published>2019-05-05T17:00:00-04:00</published><updated>2019-05-05T17:00:00-04:00</updated><author><name>Dunya Oguz</name></author><id>tag:dunyaoguz.github.io,2019-05-05:/my-blog/clustering.html</id><summary type="html">&lt;p&gt;Clustering is the act of assembling data points into distinct groups whereby each group or cluster is made up of observations that are similar to one another, and different from observations in other clusters in some fashion. Clustering falls into the category of unsupervised machine learning, where we don't explicitly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Clustering is the act of assembling data points into distinct groups whereby each group or cluster is made up of observations that are similar to one another, and different from observations in other clusters in some fashion. Clustering falls into the category of unsupervised machine learning, where we don't explicitly tell algorithms what to output, as we do in classification (is this e-mail spam or not spam?) and regression (how much does this house cost?) problems. Instead, clustering algorithms identify hidden structures and patterns in the data and reveal groupings that we didn't know existed.&lt;/p&gt;
&lt;p&gt;In this blog post, I'm going to explore and explain different clustering algorithms. Data science courses tend to only cover k-means (the hello world! of clustering), leaving the rest out to be discovered/played around with on one's own as the need to dive deeper into clustering arises, perhaps in the work place or for a particular project. Granted, once one understands the principles and underlying concepts of one clustering algorithm, the rest start to seem pretty straightforward - but i still think there is value to be gained from knowing at least the basics of more than one clustering algorithm from the get-go. 
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;K-Means Clustering&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;As mentioned, K-means is the most popular clustering algorithm. In K-means, the user specifies the number of clusters they want to create in the data - which is what we call &lt;code&gt;K&lt;/code&gt;. The algorithm takes this K, finds that many number of &lt;code&gt;centroids&lt;/code&gt; (points representing the center of their respective cluster) in the data, iterates over the entire dataset and assigns each observation to a cluster based on what centroid they are closest to, using the &lt;code&gt;Eucledian Distance&lt;/code&gt; measurement. &lt;/p&gt;
&lt;p&gt;The process of finding centroids and iterating over observations to assign them to a centroid is repeated until the best set of centroid points are found - ones which maximize the distance between clusters (&lt;code&gt;silhouette score&lt;/code&gt;) and minimize the distance between observations within clusters (&lt;code&gt;inertia&lt;/code&gt;). &lt;/p&gt;
&lt;p&gt;&lt;img src="images/K-means.gif" alt="dt" width="450"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;Fuzzy C-Means Clustering&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Fuzzy C-Means clustering is conceptually very similar to K-means clustering. Fuzzy C-means assigns observations membership probabilities of belonging to certain clusters instead of assigning each observation to a particular cluster. The notion of "x belongs to cluster 1" is thus replaced with "x belongs to cluster 1 with 80% probability, cluster 2 with 16% probability, and cluster 3 with 4% probability." Fuzzy C-Means Clustering can be implemented using the &lt;code&gt;sklearn-fuzzy&lt;/code&gt; library, which includes other algorithms using the fuzzy logic. &lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Fuzzy Partition Coefficient&lt;/code&gt; or &lt;code&gt;FCP&lt;/code&gt; in short is a metric that denotes how cleanly the data is described by a certain model. Fuzzy C-Means Clustering can be an especially useful replacement to K-Means in situations where we don't know what number K to specifiy, as we can try out a bunch of C's with Fuzzy C-Means and choose the number that delivers the highest FPC, as demonstrated below. This is equivalent to trying out a bunch of K's with K-means and seeing how well separated the clusters are in each modified version, but, is a bit more scientifically sound as we are relying on a numerical metric instead of pure visual inspection.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/clustering_4_0.png" alt="dt" width="600"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;Density-Based Spatial Clustering of Applications with Noise&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Density-Based Spatial Clustering of Applications with Noise is quite a mouthful, so it is commonly referred to as &lt;code&gt;DBSCAN&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;In DBSCAN, clusters are found by identifying areas with high density in the dataset. DBSCAN takes two user-specified inputs: &lt;code&gt;eps&lt;/code&gt;, the maximum distance two samples in the same cluster can have and  &lt;code&gt;min_samples&lt;/code&gt;, the minimum number of observations a grouping needs to include for it to be considered a cluster. With these inputs, the algorithm checks each observation in the dataset to see if there are more than &lt;code&gt;min_samples&lt;/code&gt; observations within a distance of &lt;code&gt;eps&lt;/code&gt;, until all data points in the dataset are visited. &lt;/p&gt;
&lt;p&gt;Unlike other clustering algorithms, all observations are not necessarily assigned to a cluster in DBSCAN. Those points that don't satisfy the &lt;code&gt;eps&lt;/code&gt; and &lt;code&gt;min_samples&lt;/code&gt; conditions are left out as noise points. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;img src="images/clustering_6_0.png" alt="dt" width="600"/&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;Agglomerative Hierarchical Clustering&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Agglomerative Hierarchical clustering is a clustering method which builds clusters using a &lt;code&gt;hierarchical bottom-up approach&lt;/code&gt;. At the beginning, all data points are considered to be a cluster on their own, and are successively agglomerated in groups of two based on some distance metric, until a single cluster is left. &lt;/p&gt;
&lt;p&gt;Results of agglomerative hierarchical clustering can be shown using a tree-like diagram called a &lt;code&gt;dendrogram&lt;/code&gt;, one example of which is given below. We start at the bottom where y=0 and go up sequentially as the clusters are agglomerated. The height in the dendrogram at which two clusters merge represents the distance between them prior to the merge.&lt;/p&gt;
&lt;p&gt;The point at which to stop merging clusters must be specified by the data owner based on domain knowledge, but the dendrogram can help in some cases. Though this is not a hard and fast rule:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The best choice for the number of clusters in agglomerative hierarchical clustering is the number of vertical lines in the dendrogram when cut by two horizontal lines that transverse the maximum vertical distance in the dendrogram without intersecting a cluster.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the below example, the stopping point according to this rule would be at 4 clusters.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/clustering_8_0.png" alt="dt" width="510"/&gt;
&lt;img src="images/hierarchical.png" alt="dt" width="600"/&gt;&lt;/p&gt;
&lt;h4&gt;Affinity Propagation&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;This is a really weird sounding algorithm, so bear with me. &lt;/p&gt;
&lt;p&gt;Affinity Propagation works by finding observations that are representative of the cluster they are a member of, called &lt;code&gt;exemplars&lt;/code&gt;. In Affinity Propagation, data points are seen as a network in which messages are sent back and forth between pairs of samples. Exemplars are found through this concept of message-passing whereby samples communicate their suitability to serve as the exemplar (&lt;code&gt;responsability&lt;/code&gt;) and how appropriate it would be for them to pick the sample they are messaging as their exemplar, taking into account other points' preference for that sample as an exemplar (&lt;code&gt;availability&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These messages get stored in matrices and are updated iteratively in response to messages from other pairs. In the end, observations whose responsibility and availability are positive are chosen as examplars.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/affinity.jpg" alt="dt" width="700"/&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;Mean Shift Clustering&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Mean Shift clustering is a &lt;code&gt;centroid&lt;/code&gt; based algorithm like K-means which seeks to find the center points for each cluster. It works by shifting a a circle whose radius is called &lt;code&gt;bandwidth&lt;/code&gt; iteratively through the data points to higher density regions. &lt;/p&gt;
&lt;p&gt;At every iteration, the algorithm computes the mean of all the points that fall within the circle, and shifts the center of the circle towards that mean - hence the name &lt;code&gt;mean shift&lt;/code&gt;. The shifting continues until there is no direction the circle can be shifted to that will result in more points contained within the circle. &lt;/p&gt;
&lt;p&gt;&lt;img src="images/clustering_13_0.png" alt="dt" width="600"/&gt;&lt;/p&gt;</content><category term="python"></category></entry><entry><title>Decision Trees, Ensembles of Trees and their Hyperparameters</title><link href="https://dunyaoguz.github.io/my-blog/decision-trees.html" rel="alternate"></link><published>2019-04-22T00:00:00-04:00</published><updated>2019-04-22T00:00:00-04:00</updated><author><name>Dunya Oguz</name></author><id>tag:dunyaoguz.github.io,2019-04-22:/my-blog/decision-trees.html</id><summary type="html">&lt;p&gt;Decision Trees, also referred to as CART (Classification and Regression Trees), are one of the most popular and well understood machine learning algorithms. Decision trees are super intuitive and interpretable because they mimic how the human brain works. &lt;/p&gt;
&lt;p&gt;That said, decision trees may lag behind other, more complex machine learning …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Decision Trees, also referred to as CART (Classification and Regression Trees), are one of the most popular and well understood machine learning algorithms. Decision trees are super intuitive and interpretable because they mimic how the human brain works. &lt;/p&gt;
&lt;p&gt;That said, decision trees may lag behind other, more complex machine learning algorithms (sometimes called 'black box algorithms') in accuracy. However, in many situations, like in the context of a business where you can't make certain decisions without being able to explain why (think of a bank giving out loans to individuals), interpretability is preferred over accuracy. &lt;/p&gt;
&lt;h3&gt;Decision Tree Basics&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Decision trees are essentially a bunch of &lt;strong&gt;if-then-else&lt;/strong&gt; rules stacked on top of each other. Here is a silly example:&lt;/p&gt;
&lt;p&gt;&lt;/s&gt;&lt;/p&gt;
&lt;h4&gt;&lt;center&gt; Question: Should I write this blog post? &lt;/center&gt;&lt;/h4&gt;
&lt;p&gt;&lt;/s&gt;
&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/CART_3_0.png" alt="dt" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;/s&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"&lt;em&gt;Do I understand the concept?&lt;/em&gt;" is the &lt;strong&gt;root node&lt;/strong&gt;. &lt;/li&gt;
&lt;li&gt;"&lt;em&gt;Am I tired?&lt;/em&gt;" and "&lt;em&gt;Is there coffee?&lt;/em&gt;" are &lt;strong&gt;internal nodes&lt;/strong&gt;. &lt;/li&gt;
&lt;li&gt;The colored boxes are &lt;strong&gt;leaf nodes&lt;/strong&gt; and they store the target variable, i.e. the prediction made by the algorithm.&lt;/li&gt;
&lt;li&gt;The arrows pointing from internal nodes to leaf nodes are &lt;strong&gt;branches&lt;/strong&gt;. &lt;/li&gt;
&lt;li&gt;Nodes that precede other nodes are called &lt;strong&gt;parent nodes&lt;/strong&gt;. Nodes that immediately proceed them are referred to as their &lt;strong&gt;children&lt;/strong&gt;. "&lt;em&gt;Am I tired&lt;/em&gt;?" and "&lt;em&gt;Don't write blog&lt;/em&gt;" are the children of "&lt;em&gt;Do I understand the concept?&lt;/em&gt;". &lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;depth&lt;/strong&gt; of a tree is length of the longest path from the root node to a leaf node. The decision tree above has a depth of 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decision tree algorithm operates by coming up with rules and partitioning the data accordingly at each node in a &lt;strong&gt;sequential&lt;/strong&gt; manner. But how does the algorithm figure out the logical order of rules? In the above case, how can it know that my understanding of the concept is the number one question that must be answered yes, before any other thing such as whether or not there is coffee?&lt;/p&gt;
&lt;p&gt;Answer: with the help of some good old math! Decision trees are built in a top-down fashion through finding the attribute that maximizes something called &lt;strong&gt;information gain&lt;/strong&gt; at each split. The idea is that, the higher the information gain, the higher the importance of the attribute in determining the outcome/target variable. Decision trees are often called a &lt;strong&gt;greedy&lt;/strong&gt; algorithm since they find the best partition each individual step, rather than optimizing for a global tree structure that will deliver the best prediction.&lt;/p&gt;
&lt;p&gt;The formula for information gain is as follows:&lt;/p&gt;
&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;$$ Information&lt;/br&gt;Gain = E(\text{parent})  - \sum_{\text{children}}\frac{N_j}{N}E(\text{children}_j) $$&lt;/p&gt;
&lt;p&gt;This can look intimidating at first but it's actually quite simple. The function E() is something called &lt;strong&gt;entropy&lt;/strong&gt;. To compute information gain, we simply deduct the weighted sum of the entropies of the children nodes from the entropy of the parent node. The attribute that yields the highest information gain when partioned to its children gets chosen as the root node. This process continues with the remaining subset of data until there is no way to further partition the tree, which is when there is one class left in each leaf node. When that is the case, the decision tree is considered complete. &lt;/p&gt;
&lt;p&gt;Entropy measures the amount of unpredictability in a random variable. In practical terms, it is a measure of &lt;strong&gt;impurity&lt;/strong&gt; of an attribute. If the attribute in question is composed of a single class, for example, all yes's or all no's, then that attribute is considered &lt;strong&gt;pure&lt;/strong&gt; and entropy takes on a value of 0. If the classes are equally distributed, for example, 50% yes's and 50% no's, entropy takes on a value of 1. &lt;/p&gt;
&lt;p&gt;Here is the formula for entropy:&lt;/p&gt;
&lt;p&gt;$$ Entropy = \sum_{i=1}^{classes} - {P_i} * {log_2}{P_i} $$&lt;/p&gt;
&lt;p&gt;Entropy and information gain are used interchangeably with something called &lt;strong&gt;gini index&lt;/strong&gt;, a slightly different measure of node impurity. They yield very similar results. &lt;/p&gt;
&lt;p&gt;To better understand how decision trees work, let's manually rebuild one. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn_pandas import DataFrameMapper
from sklearn.preprocessing import LabelEncoder, LabelBinarizer
from sklearn.model_selection import train_test_split

blog_post = pd.DataFrame({'conceptual_understanding': ['No', 'No', 'No', 'No', 'Yes', 
                                                       'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],
                          'am_i_tired': ['No', 'No', 'No', 'Yes', 'No', 
                                         'No', 'No', 'Yes', 'Yes', 'Yes'],
                          'is_there_coffee': ['No', 'No', 'No', 'No', 'No', 
                                              'No', 'No','Yes', 'Yes', 'No'],
                          'write_blog': ['No', 'No', 'No', 'No', 'Yes', 
                                         'Yes', 'Yes', 'Yes', 'Yes', 'No']
                         })
# preprocessing the data
mapper = DataFrameMapper([
    ('conceptual_understanding', LabelEncoder()),
    ('am_i_tired', LabelEncoder()),
    ('is_there_coffee', LabelEncoder()),
    ('write_blog', LabelEncoder())
], df_out=True)
blog_post = mapper.fit_transform(blog_post)
X = blog_post.iloc[:, :-1]
y = blog_post['write_blog']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)
&lt;/pre&gt;

&lt;p&gt;Check out &lt;a href="https://dunyaoguz.github.io/my-blog/dataframemapper.html"&gt;my previous blog post&lt;/a&gt; on DataFrameMapper for more information on how it works.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
# instantiate the Decision Tree
dt = DecisionTreeClassifier(criterion='entropy')
dt.fit(X_train, y_train)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DecisionTreeClassifier(class_weight=None, criterion=&amp;#39;entropy&amp;#39;, max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter=&amp;#39;best&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;pre class="prettyprint"&gt;
dt.score(X_test, y_test)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A perfect score! Our Decision Tree Classifier can predict whether I'm going to write a blog post or not with 100% accuracy. Let's see how the algorithm partitioned the data.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
from IPython.display import Image  
import pydotplus
from sklearn import tree

dot_data = tree.export_graphviz(
    dt, 
    out_file=None,
    filled=True,
    rounded=True,
    feature_names=X_train.columns,
    class_names=['Don\'t write', 'Write'])

graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png(), width=300)
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="png" src="images/CART_11_0.png"&gt;&lt;/p&gt;
&lt;p&gt;We see that the algorithm was able to produce the exact same order of rules I came up with through logic, with math. Here, we can also see the number of samples that got partitioned at each node, along with the entropies that were calculated. Now, let's redo this manually.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step one:&lt;/strong&gt; We have 100% of the train data (8 samples). Our train dataset includes 3 variables: &lt;code&gt;conceptual_understanding&lt;/code&gt;, &lt;code&gt;am_i_tired&lt;/code&gt; and &lt;code&gt;is_there_coffee&lt;/code&gt;. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
train_data = X_train.merge(y_train, how='inner', on=X_train.index).set_index('key_0')
HTML(train_data.head().to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;conceptual_understanding&lt;/th&gt;
      &lt;th&gt;am_i_tired&lt;/th&gt;
      &lt;th&gt;is_there_coffee&lt;/th&gt;
      &lt;th&gt;write_blog&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;key_0&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre class="prettyprint"&gt;
# Define Entropy function.
def entropy(y):
    y = list(y)
    unique_elements = set(y)
    entropy_sum = []
    # Iterate through each class.
    for element in unique_elements:
        # Calculate observed probability of class i
        prob = (y.count(element) / len(y))
        # Perform the entropy formula
        entropy_sum.append(prob * np.log2(prob))
    return -sum(entropy_sum)

# Define Information Gain function.
def information_gain(df, column):
    # Calculate parent_entropy
    parent_entropy = entropy(df['write_blog'])
    # Calculate the weighted sum of child entropies
    child_0_entropy = entropy(df[df[column] == 0]['write_blog'])
    child_1_entropy = entropy(df[df[column] == 1]['write_blog'])
    child_0_count = len(df[df[column] == 0]['write_blog'])
    child_1_count = len(df[df[column] == 1]['write_blog'])
    child_entropy_weighted_sum = child_0_entropy * child_0_count/len(df['write_blog']) + child_1_entropy * child_1_count/len(df['write_blog'])
    # Return information gain
    return parent_entropy - child_entropy_weighted_sum
&lt;/pre&gt;

&lt;p&gt;Let's calculate the information gain for each 3 variables at the root node.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
print(f'Parent entropy: {entropy(train_data.write_blog)}\n')
print('Information gain at the root node:\n')
a = information_gain(train_data, 'conceptual_understanding')
b = information_gain(train_data, 'am_i_tired')
c = information_gain(train_data, 'is_there_coffee')
print(f'\t1. conceptual_understanding: {round(a, 3)}')
print(f'\t2. am_i_tired: {round(b, 3)}')
print(f'\t3. is_there_coffee: {round(c, 3)}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Parent entropy: 1.0

Information gain at the root node:

    1. conceptual_understanding: 0.549
    2. am_i_tired: 0.049
    3. is_there_coffee: 0.138
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since &lt;code&gt;conceptual_understanding&lt;/code&gt; yields the highest information gain, we choose it as the root node. The 3 rows for which &lt;code&gt;conceptual_understanding&lt;/code&gt; is no assume a target value of &lt;code&gt;don't write&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; We recalculate information gain for the other 5 rows for which &lt;code&gt;conceptual_understanding&lt;/code&gt; is yes.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
# get the remaining data
remaining_data = train_data[train_data['conceptual_understanding'] != 0]

# calculate information gain for the remaining data
print(f'Parent entropy: {round(entropy(remaining_data.write_blog), 3)}\n')
print('Information gain at the first internal node:\n')
a = information_gain(remaining_data, 'conceptual_understanding')
b = information_gain(remaining_data, 'am_i_tired')
c = information_gain(remaining_data, 'is_there_coffee')
print(f'\t1. conceptual_understanding: {round(a, 3)}')
print(f'\t2. am_i_tired: {round(b, 3)}')
print(f'\t3. is_there_coffee: {round(c, 3)}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Parent entropy: 0.722

Information gain at the first internal node:

    1. conceptual_understanding: 0.0
    2. am_i_tired: 0.322
    3. is_there_coffee: 0.073
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice how information gain for &lt;code&gt;conceptual_understanding&lt;/code&gt; is now 0, since all rows have a value of yes. In non-math terms, this variable is no longer useful in determining the outcome of whether I write the blog post or not. Since &lt;code&gt;am_i_tired&lt;/code&gt; has the highest information gain here, we select it as the first internal node. The 3 rows for which &lt;code&gt;am_i_tired&lt;/code&gt; is no assume a target value of &lt;code&gt;write&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; We recalculate information gain for the remaining 2 rows, where both &lt;code&gt;conceptual_understanding&lt;/code&gt; and &lt;code&gt;am_i_tired&lt;/code&gt; are yes. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
# get the remaining data
remaining_data_2 = remaining_data[remaining_data['am_i_tired'] != 0]

# calculate information gain for the remaining data
print(f'Parent entropy: {entropy(remaining_data_2.write_blog)}\n')
print('Information gain at the second internal node:\n')
a = information_gain(remaining_data_2, 'conceptual_understanding')
b = information_gain(remaining_data_2, 'am_i_tired')
c = information_gain(remaining_data_2, 'is_there_coffee')
print(f'\t1. conceptual_understanding: {round(a, 3)}')
print(f'\t2. am_i_tired: {round(b, 3)}')
print(f'\t3. is_there_coffee: {round(c, 3)}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Parent entropy: 1.0

Information gain at the second internal node:

    1. conceptual_understanding: 0.0
    2. am_i_tired: 0.0
    3. is_there_coffee: 1.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, &lt;code&gt;is_there_coffee&lt;/code&gt; has the highest information gain because the other two variables no longer impact the outcome. If &lt;code&gt;is_there_coffee&lt;/code&gt; is no, the target variable is &lt;code&gt;don't write&lt;/code&gt;. If &lt;code&gt;is_there_coffee&lt;/code&gt; is yes, the target variable is &lt;code&gt;write&lt;/code&gt;. Since there is no way to further partition the tree at this point, our decision tree is complete.&lt;/p&gt;
&lt;p&gt;This example illustrated how decision trees work with a binary classification problem, but the principles and concepts shown here apply to decision tree regressors as well. &lt;/p&gt;
&lt;h3&gt;Hyperparameters of Decision Trees&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Now that we understand how decision trees work, let's talk about their &lt;strong&gt;hyperparameters&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In machine learning, &lt;strong&gt;hyperparameters&lt;/strong&gt; are built in model configurations whose values are specified before the learning process begins. They are independent from the data on which the model is fit and can be &lt;em&gt;tuned&lt;/em&gt; as desired. &lt;/p&gt;
&lt;p&gt;Hyperparameters can dramatically change the performance of a model, and finding the combination of hyperparameters that yield the best performance is a common component of machine learning workflows. &lt;/p&gt;
&lt;p&gt;Let's examine the hyperparameters available in decision trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;criterion&lt;/code&gt;: Measure of impurity that will be used to partition the data. Options = [gini, entropy]. Default = gini.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;splitter&lt;/code&gt;: The strategy used to split the decision tree at each node. If this is set to random, a random value is partitioned at each node instead of the attribute that yields the highest information gain or gini impurity. Options = [best, random]. Default = best.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;max_depth&lt;/code&gt;: The maximum length of the longest path from the root node to a leaf node. Unless max depth is specified, decision tree will automatically extend until all leaf nodes are pure or contain less than &lt;code&gt;min_samples_split&lt;/code&gt; samples. Options = [None, int]. Default = None.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;min_samples_split&lt;/code&gt;: The minimum number of samples required to partition a node. If we had set &lt;code&gt;min_samples_split&lt;/code&gt; to 3 when we instantiated the model above, the data would not be partitioned at &lt;code&gt;is_there_coffee&lt;/code&gt; since only 2 samples remained at that node. Options = [int, float]. Default = 2.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;min_samples_leaf&lt;/code&gt;: The minimum number of samples required to be a leaf node. Data will not be partitioned at a node if any of its children to the right or left would have less samples than min_samples_leaf. As an example, if I had specified &lt;code&gt;min_samples_leaf&lt;/code&gt; as 2, the data would not be split at &lt;code&gt;is_there_coffee&lt;/code&gt; because both of its children have 1 sample. Options = [int, float]. Default = 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;max_features&lt;/code&gt;: The maximum number of features to consider when looking for the best split. In the above example, we calculated information gain for all 3 features in the dataset at each partition. If &lt;code&gt;max_features&lt;/code&gt; was specified as 2, information gain would be computed for only 2 of the features at each split. Options = [None, int, string, float]. Default = None.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;max_leaf_nodes&lt;/code&gt;: Maximum number of leaf nodes that can exist in the decision tree. If it is specified, the decision tree stops partitioning once it reaches the set number of leaf nodes. Options = [None, int]. Default = None.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;min_impurity_decrease&lt;/code&gt;: Minimum amount of decrease in impurity that needs to be induced by partitioning a node. If the decrease in impurity would be less than &lt;code&gt;min_impurity_decrease&lt;/code&gt;, the said partition does not occur. Options = [None, float]. Default = None.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tree based ensemble methods&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Decision trees tend to suffer from overfitting and perform poorly on unseen data. Tree based ensemble methods combine several decision trees in order to improve the predictive performance of stand alone decision trees. Here, I'm going to talk about two techniques: &lt;strong&gt;bootstrap aggregated trees&lt;/strong&gt; and &lt;strong&gt;random forests&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Bootstrap aggregated trees, or bagging trees in short, withdraw several random samples from the dataset with replacement, and train a decision tree on each sample. When given a new dataset, bagging trees calculate the predictions by averaging the results from each bootstrapped decision tree. &lt;/p&gt;
&lt;p&gt;Random forests also build various decision trees on bootstrapped resamples from the original dataset, but differently than bagged trees, random forests only consider a subset of the features at each iteration. With bagging, the decision trees in the aggregation tend to be strongly correlated to each other since each tree includes all of the original features. The selection of features at random (called the &lt;strong&gt;random subspace method&lt;/strong&gt;) in random forests counters the correlation between trees, resulting in an overall model that performs better on unseen data. &lt;/p&gt;
&lt;h3&gt;Hyperparameters of Bagging Trees and Random Forests&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The hyperparameters of bagging trees and random forests are exactly the same as the hyperparameters of decision trees, with a few additions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;n_estimators&lt;/code&gt;: The number of trees in the ensemble. Options = [int]. Default = 10. (The default option for Random Forest will change to 100 in the new version of sklearn.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;bootstrap&lt;/code&gt;: Whether samples are drawn with replacement. Options = [True, False]. Default = True. This argument should never be set to False as drawing samples without replacement would be equivalent to duplicating the original dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;n_jobs&lt;/code&gt;: The number of processors to run while training the model and computing predictions from it. If set to 1, no parallel computing code is used. If set to -1, all CPUs are used. Options = [None, int]. Default = None.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;oob_score&lt;/code&gt;: On average, each bootstrapped sample uses about 2/3 of the observations. The remaining 1/3 of the observations not used to fit a given bootstrapped tree are referred to as the out-of-bag (OOB) observations. If this argument is set to true, OOB observations are used to score the performance of their respective bootstrapped tree. Options = [True, False]. Default = False.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;verbose&lt;/code&gt;: If set to true, the output for each decision tree in the ensemble gets printed. Options = [True, False]. Default = False.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="python"></category></entry><entry><title>DataFrameMapper: A cleaner, more robust way to preprocess data</title><link href="https://dunyaoguz.github.io/my-blog/dataframemapper.html" rel="alternate"></link><published>2019-04-13T17:00:00-04:00</published><updated>2019-04-13T17:00:00-04:00</updated><author><name>Dunya Oguz</name></author><id>tag:dunyaoguz.github.io,2019-04-13:/my-blog/dataframemapper.html</id><summary type="html">&lt;p&gt;Data must be cleaned and put in a particular shape and form prior to applying machine learning models. This task is referred to as "data preprocessing" and is the first step in any data science and machine learning workflow. &lt;/p&gt;
&lt;p&gt;It's no secret that data preprocessing is a dull, mundane activity …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Data must be cleaned and put in a particular shape and form prior to applying machine learning models. This task is referred to as "data preprocessing" and is the first step in any data science and machine learning workflow. &lt;/p&gt;
&lt;p&gt;It's no secret that data preprocessing is a dull, mundane activity. Not only does it not require much brain energy (for the most part), but it can also get quite repetitive. To make matters worse, data preprocessing is said to constitute 80% of most data scientists' working time. &lt;/p&gt;
&lt;p&gt;&lt;img src="images/DataFrameMapper_1_0.jpeg" alt="dt"/&gt;&lt;/p&gt;
&lt;p&gt;One can safely say that not many data scientists enjoy data preprocessing, as demonstrated by the cartoon above. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enter DataFrameMapper.&lt;/strong&gt; Fortunately, data preprocessing doesn't have to be as tedious as some Kaggle competitors I've seen have made it out to be (!). With DataFrameMapper, code that can clean and transform thousands, millions of rows of data can be written in a very concise, robust and standardized fashion.&lt;/p&gt;
&lt;p&gt;Let's dive deeper into what DataFrameMapper is and how it can be used to make data preprocessing a little bit less dreadful. &lt;/p&gt;
&lt;h3&gt;What is DataFrameMapper?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;DataFrameMapper is a module in an experimental scikit-learn library called sklearn-pandas, developed to bridge the gap between actual pandas and scikit-learn, the two most common technologies in a data scientist's toolkit. &lt;/p&gt;
&lt;p&gt;The traditional way of doing things in data science is to clean and prepare the data in pandas, and then pass it on to scikit-learn to apply machine learning models. There are modules available in both pandas and scikit-learn to handle common data preprocessing tasks like imputing nulls and turning categorical columns into numeric values. But, without a standardized way to perform these tasks, the procedure of data preprocessing can get quite fragmented and messy, which becomes a problem when new data needs to be fed to a machine learning model. &lt;/p&gt;
&lt;p&gt;DataFrameMapper enables all the steps of data preprocessing to be grouped together and stored in a single object, and applied to any dataset with a single operation. &lt;/p&gt;
&lt;h3&gt;How does it work?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;DataFrameMapper maps preprocessing tasks to each column of a given dataset via a list of tuples. Each tuple in the input list refers to a specific column of the dataframe. The first element in the tuple takes the name of the column, and the second element takes the preprocessing task or tasks that want to be applied to that particular column. If there is more than one task, the second element of the tuple needs to be a list, the order of which needs to match the desired order of operations. &lt;/p&gt;
&lt;p&gt;Let's see how DataFrameMapper works with an example. First, &lt;code&gt;pip install sklearn-pandas&lt;/code&gt;, and import it onto your workspace as follows.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
#!pip install sklearn-pandas
from sklearn_pandas import DataFrameMapper

# other imports
from IPython.display import HTML
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")
&lt;/pre&gt;

&lt;p&gt;I'm going to spin up a dataframe of my favorite tv shows, including information on their production cost (made up), number of seasons, mainstream popularity score out of 10 (made up), genre and whether or not they are on netflix.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
my_favorite_shows = {
    'name': ['sense_8', 
             'handmaidens_tale', 
             'the_good_place', 
             'big_little_lies', 
             'jane_the_virgin', 
             'game_of_thrones', 
             'mad_men', 
             'the_crown', 
             'narcos', 
             'house_of_cards', 
             'girls', 
             'breaking_bad', 
             'bad_blood', 
             'fauda', 
             'jessica_jones'],
    'cost': [None, 
             140000000, 
             80000000, 
             170000000, 
             205000000, 
             600000000, 
             300000000, 
             None, 
             400000000, 
             500000000, 
             112000000, 
             380000000, 
             10000000, 
             75000000, 
             None],
    'seasons': [2, None, 3, 1, 5, 9, 7, 2, 3, 5, 6, 5, 2, None, 2],
    'popularity': [5.8, 6, 5.7, 7.3, 6.5, 9.8, 8.4, 
                   7.6, 8, 9.3, 7, 8.9, 2.3, 5.2, 4.7],
    'genre': ['science_fiction', 
              'speculative_fiction', 
              'comedy', 
              'drama', 
              'comedy', 
              'fantasy', 
              'period_drama', 
              'period_drama', 
              'period_drama', 
               None, 
              'comedy', 
              'crime', 
              'crime', 
              'crime', 
              'science_fiction'],
    'on_netflix': ['yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 
                   'yes', None, 'yes', 'no', 'yes', None, 'yes', 'yes']
}

my_favorite_shows = pd.DataFrame(my_favorite_shows)
HTML(my_favorite_shows.head(10).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;seasons&lt;/th&gt;
      &lt;th&gt;popularity&lt;/th&gt;
      &lt;th&gt;genre&lt;/th&gt;
      &lt;th&gt;on_netflix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;sense_8&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;5.8&lt;/td&gt;
      &lt;td&gt;science_fiction&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;handmaidens_tale&lt;/td&gt;
      &lt;td&gt;140000000.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;speculative_fiction&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;the_good_place&lt;/td&gt;
      &lt;td&gt;80000000.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;5.7&lt;/td&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;big_little_lies&lt;/td&gt;
      &lt;td&gt;170000000.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;7.3&lt;/td&gt;
      &lt;td&gt;drama&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;jane_the_virgin&lt;/td&gt;
      &lt;td&gt;205000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;6.5&lt;/td&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;game_of_thrones&lt;/td&gt;
      &lt;td&gt;600000000.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;fantasy&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;mad_men&lt;/td&gt;
      &lt;td&gt;300000000.0&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;8.4&lt;/td&gt;
      &lt;td&gt;period_drama&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;the_crown&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;7.6&lt;/td&gt;
      &lt;td&gt;period_drama&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;narcos&lt;/td&gt;
      &lt;td&gt;400000000.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;period_drama&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;house_of_cards&lt;/td&gt;
      &lt;td&gt;500000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;9.3&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let's say I want to predict the popularity score of a tv show using its production cost, genre, number of seasons and whether or not it's on netflix. Before I can train a machine learning model with the data I have, I need to get rid of all the NaN values that I intentionally put, and encode all categorical attributes as numbers. &lt;/p&gt;
&lt;p&gt;Let's see how preprocessing this dataset would look like &lt;em&gt;without&lt;/em&gt; using DataFrameMapper. &lt;/p&gt;
&lt;p&gt;Splitting our dataset into two - data with which we will train our model and data with which we will test the performance of our model - is the first thing we need to do.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
from sklearn.model_selection import train_test_split
X = my_favorite_shows.drop(columns=['name', 'popularity'], axis=1)
y = my_favorite_shows['popularity']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)
HTML(X_train.head(10).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;seasons&lt;/th&gt;
      &lt;th&gt;genre&lt;/th&gt;
      &lt;th&gt;on_netflix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;600000000.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;fantasy&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;170000000.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;drama&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;500000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;400000000.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;period_drama&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;140000000.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;speculative_fiction&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;205000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;science_fiction&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;300000000.0&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;period_drama&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;75000000.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;crime&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;112000000.0&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Next, we need to get rid of all the nulls. I'll fill the null values in numerical columns with the median value for the respective column and the nulls in categorical columns with 'unknown'.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
median_budget = X_train['cost'].quantile(0.5)
median_season = X_train['seasons'].quantile(0.5)
X_train['cost'] = X_train['cost'].fillna(median_budget)
X_train['genre'] = X_train['genre'].fillna('unknown')
X_train['seasons'] = X_train['seasons'].fillna(median_season)
X_train['on_netflix'] = X_train['on_netflix'].fillna('unknown')
&lt;/pre&gt;

&lt;p&gt;I need to transform the genre column to numeric values. A common way to do this is with the &lt;code&gt;LabelBinarizer&lt;/code&gt; function from sklearn, which creates a column for each unique value in a category, and represents membership with 1s and 0s. (1 for members, 0 for non members)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT ADVICE&lt;/strong&gt;: Do NOT use the &lt;code&gt;get_dummies()&lt;/code&gt; function from pandas to encode your categorical variables! Things will break apart and your model will not work if the categories in your test data does not match the categories in your training data, which is a &lt;strong&gt;VERY&lt;/strong&gt; common occurance!&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
from sklearn.preprocessing import LabelBinarizer
lb = LabelBinarizer()
binarized = pd.DataFrame(lb.fit_transform(X_train['genre']), columns=list(lb.classes_))
HTML(binarized.head(5).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;comedy&lt;/th&gt;
      &lt;th&gt;crime&lt;/th&gt;
      &lt;th&gt;drama&lt;/th&gt;
      &lt;th&gt;fantasy&lt;/th&gt;
      &lt;th&gt;period_drama&lt;/th&gt;
      &lt;th&gt;science_fiction&lt;/th&gt;
      &lt;th&gt;speculative_fiction&lt;/th&gt;
      &lt;th&gt;unknown&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I now have to add these label binarized columns to my X_train, and remove the original genre column.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
X_train.drop(columns=['genre'], axis=1, inplace=True)
Z_train = pd.merge(X_train, binarized, how='left', on = X_train.index)
HTML(Z_train.head(5).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;key_0&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;seasons&lt;/th&gt;
      &lt;th&gt;on_netflix&lt;/th&gt;
      &lt;th&gt;comedy&lt;/th&gt;
      &lt;th&gt;crime&lt;/th&gt;
      &lt;th&gt;drama&lt;/th&gt;
      &lt;th&gt;fantasy&lt;/th&gt;
      &lt;th&gt;period_drama&lt;/th&gt;
      &lt;th&gt;science_fiction&lt;/th&gt;
      &lt;th&gt;speculative_fiction&lt;/th&gt;
      &lt;th&gt;unknown&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;600000000.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;170000000.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;500000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;400000000.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;unknown&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;140000000.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Since the index of X_train turned to a column (key_0) during the merge, I need to reset it back to it's original state.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
Z_train.set_index('key_0', inplace=True)
HTML(Z_train.head(5).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;seasons&lt;/th&gt;
      &lt;th&gt;on_netflix&lt;/th&gt;
      &lt;th&gt;comedy&lt;/th&gt;
      &lt;th&gt;crime&lt;/th&gt;
      &lt;th&gt;drama&lt;/th&gt;
      &lt;th&gt;fantasy&lt;/th&gt;
      &lt;th&gt;period_drama&lt;/th&gt;
      &lt;th&gt;science_fiction&lt;/th&gt;
      &lt;th&gt;speculative_fiction&lt;/th&gt;
      &lt;th&gt;unknown&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;key_0&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;600000000.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;170000000.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;500000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;400000000.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;unknown&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;140000000.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I also need to encode the on_netflix column as numbers. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
Z_train['on_netflix'] = Z_train['on_netflix'].replace({'no': 0, 'yes': 2, 'unknown': 1})
&lt;/pre&gt;

&lt;p&gt;The data is finally ready for modelling. Let's create a simple linear regression model and try to predict popularity scores.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(Z_train, y_train)
train_score = model.score(Z_train, y_train)
print(f'Train score: {train_score}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Train score: 0.9791971116650907
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Apparently, our simple linear regression model is able to predict ~98% of the variability in popularity score. Of course, this score is only based on the train dataset - to evaluate the true performance of our regression model, we need to score it on our test data.&lt;/p&gt;
&lt;p&gt;I now have go back and replicate everything I did on the training data on the test data in order to be able to pass it onto my model. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
X_test['cost'] = X_test['cost'].fillna(median_budget)
X_test['genre'] = X_test['genre'].fillna('unknown')
X_test['seasons'] = X_test['seasons'].fillna(median_season)
X_test['on_netflix'] = X_test['on_netflix'].fillna('unknown')
binarized = pd.DataFrame(lb.transform(X_test['genre']), columns=list(lb.classes_))
X_test.drop(columns=['genre'], axis=1, inplace=True)
Z_test = pd.merge(X_test, binarized, how='left', on = X_test.index)
Z_test['on_netflix'] = Z_test['on_netflix'].replace({'no': 0, 'yes': 2, 'unknown': 1})
Z_test.set_index('key_0', inplace=True)
&lt;/pre&gt;

&lt;pre class="prettyprint"&gt;
test_score = model.score(Z_test, y_test)
print(f'Test score: {test_score}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test score: 0.43538804012831567
&lt;/pre&gt;&lt;/div&gt;


&lt;pre class="prettyprint"&gt;
print(f'Predicted scores: {list(model.predict(Z_test))}')
print(f'Actual scores: {list(y_test)}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Predicted scores: [6.937697253068383, 4.820338983050848, 6.25195791934541]
Actual scores: [7.6, 2.3, 8.9]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As expected, the model was grossly overfit and the performance of the model on the test dataset is pretty bad. (Not a concern since our data is fake and we are not trying to build an actual model here.)&lt;/p&gt;
&lt;p&gt;Let's see how much more easily reproducible data preprocessing would be had we used DataFrameMapper.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
from sklearn.impute import SimpleImputer
from sklearn_pandas import CategoricalImputer
from sklearn.preprocessing import LabelEncoder
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)
mapper = DataFrameMapper([
    (['cost'], SimpleImputer(strategy='median')),
    (['seasons'], SimpleImputer(strategy='median')),
    ('genre', [CategoricalImputer(strategy='constant', fill_value='unknown'),
               LabelBinarizer()]),
    ('on_netflix', [CategoricalImputer(strategy='constant', fill_value='unknown'),
                   LabelEncoder()])
], df_out=True)
Z_train = mapper.fit_transform(X_train)
HTML(Z_train.head(5).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;seasons&lt;/th&gt;
      &lt;th&gt;genre_comedy&lt;/th&gt;
      &lt;th&gt;genre_crime&lt;/th&gt;
      &lt;th&gt;genre_drama&lt;/th&gt;
      &lt;th&gt;genre_fantasy&lt;/th&gt;
      &lt;th&gt;genre_period_drama&lt;/th&gt;
      &lt;th&gt;genre_science_fiction&lt;/th&gt;
      &lt;th&gt;genre_speculative_fiction&lt;/th&gt;
      &lt;th&gt;genre_unknown&lt;/th&gt;
      &lt;th&gt;on_netflix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;600000000.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;170000000.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;500000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;400000000.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;140000000.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre class="prettyprint"&gt;
Z_test = mapper.transform(X_test)
HTML(Z_test.head(3).to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;seasons&lt;/th&gt;
      &lt;th&gt;genre_comedy&lt;/th&gt;
      &lt;th&gt;genre_crime&lt;/th&gt;
      &lt;th&gt;genre_drama&lt;/th&gt;
      &lt;th&gt;genre_fantasy&lt;/th&gt;
      &lt;th&gt;genre_period_drama&lt;/th&gt;
      &lt;th&gt;genre_science_fiction&lt;/th&gt;
      &lt;th&gt;genre_speculative_fiction&lt;/th&gt;
      &lt;th&gt;genre_unknown&lt;/th&gt;
      &lt;th&gt;on_netflix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;187500000.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;10000000.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;380000000.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;That's it, friends!&lt;/strong&gt; We were able to do everything that was done previously with just 5-6 lines of code. The best part is that now, I can transform any new data to a model-ready state with a single line of code.&lt;/p&gt;
&lt;p&gt;Let's pretend like we have to predict the popularity score of a new show, not included in our original dataset. Without DataFrameMapper, we would have to repeat the previous preprocessing steps for a third time. With DataFrameMapper, we can just pass the new data onto mapper.transform, and immediately get a prediction.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
new_data = {'name': ['the_protector'],
            'cost': [5_000_000],
            'seasons': [1],
            'genre': ['science_fiction'],
            'on_netflix': ['yes']}
new_data = pd.DataFrame(new_data)
new_Z = mapper.transform(new_data)
print(f'Predicted popularity score: {round(float(model.predict(new_Z)), 3)}')
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Predicted popularity score: 9.885
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;Notice that:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We fit transform the training data, but only transform the test data and the data for which we want to get a prediction. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;df_out=True&lt;/code&gt; argument enables us to get a dataframe output from the transform function. By default, &lt;code&gt;df_out&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, so if we don't include it in the mapper we would get a numpy array as the output. Either is fine as far modelling goes, it's more a matter of convenience. I personally prefer seeing pandas dataframes over numpy arrays as I find them easier to read.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;LabelEncoder&lt;/code&gt; transformer replaces categorical variables with numerical labels, like the &lt;code&gt;pd.replace&lt;/code&gt; function used previously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When using &lt;code&gt;SimpleImputer&lt;/code&gt; - which is a sklearn imputation transformer for numeric values - we have to wrap the first element of the tuple, i.e., the column name, within brackets. Otherwise, the mapper will throw an error. This has to do with the fact that &lt;code&gt;SimpleImputer&lt;/code&gt; needs to take lists as input. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I passed two transformers to the genre and on_netflix columns - the &lt;code&gt;CategoricalImputer&lt;/code&gt; first, followed by the &lt;code&gt;LabelBinarizer&lt;/code&gt; in the case of genre and &lt;code&gt;LabelEncoder&lt;/code&gt; in the case of on_netflix. If I had done the reverse, the mapper would throw an error because null values can't be label binarized. &lt;strong&gt;As a rule of thumb and general good practice, imputers need to be the first transformation in a mapper and they need to be applied to &lt;em&gt;all&lt;/em&gt; columns.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I imported &lt;code&gt;CategoricalImputer&lt;/code&gt; from &lt;code&gt;pandas&lt;/code&gt; whereas I imported &lt;code&gt;SimpleImputer&lt;/code&gt; from &lt;code&gt;sklearn.impute&lt;/code&gt;. This is another great thing about &lt;code&gt;sklearn-pandas&lt;/code&gt;: they provide functionality for the imputation of categorical values, which traditionally did not exist in sklearn. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The strategy argument in the imputation transformers lets us decide &lt;em&gt;what&lt;/em&gt; we want to replace the null values with. Available strategies in &lt;code&gt;SimpleImputer&lt;/code&gt; are median, mean, mode or any constant value of choice. Available strategies in &lt;code&gt;CategoricalImputer&lt;/code&gt; are the most frequent value or a constant of choice. If strategy is set to constant, the &lt;code&gt;fill_value&lt;/code&gt; argument also needs to be defined, as I have done above.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Happy mappin'! :)&lt;/h3&gt;</content><category term="python"></category></entry><entry><title>Matplotlib Hacks</title><link href="https://dunyaoguz.github.io/my-blog/matplotlib_hacks.html" rel="alternate"></link><published>2019-03-30T00:00:00-04:00</published><updated>2019-03-30T00:00:00-04:00</updated><author><name>Dunya Oguz</name></author><id>tag:dunyaoguz.github.io,2019-03-30:/my-blog/matplotlib_hacks.html</id><summary type="html">&lt;p&gt;New to python and struggling to understand Matplotlib? Actually scratch the first part. Struggling to understand Matplotlib &lt;em&gt;period&lt;/em&gt;? You are &lt;strong&gt;NOT&lt;/strong&gt; alone my friend. &lt;/p&gt;
&lt;p&gt;Let's get some things straight. Matplotlib's design choices are ... let's just say ... not the most straightforward. Weird. Truly incomprehensible sometimes. It is also poorly documented and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;New to python and struggling to understand Matplotlib? Actually scratch the first part. Struggling to understand Matplotlib &lt;em&gt;period&lt;/em&gt;? You are &lt;strong&gt;NOT&lt;/strong&gt; alone my friend. &lt;/p&gt;
&lt;p&gt;Let's get some things straight. Matplotlib's design choices are ... let's just say ... not the most straightforward. Weird. Truly incomprehensible sometimes. It is also poorly documented and at times inconsistent. There's people out there who have been working with Python for years  and still can't wrap their heads around Matplotlib. &lt;/p&gt;
&lt;p&gt;In my DSI cohort at General Assembly, there seems to be two camps of people: those who hate Matplotlib and have pretty much given up on it (everyone except me) vs those who think it is not so bad (me). I've strangely come to appreciate some of Matplotlib's functionalities and found myself reaching back to it again and again even though I now have sexy new libraries like Altair under my belt. In this blog post, I am going to give some tips, tricks and general information that has been useful for me in my data viz journey with Matplotlib. I hope I'll be able to convert some of my hardline anti-Matplotlib class mates to the "matplotlib is not so bad! let's love and appreciate it!" camp. After all, matplotlib is the foundation upon which the bulk of python's data visualisation libraries are built. &lt;/p&gt;
&lt;h3&gt;Making pretty charts&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;We can all probably agree that Matplotlib's default charting style is really ugly. Let's see how we can make simple Matplotlib charts a bit easier on the eyes with a few simple lines of code. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from IPython.display import HTML

# spin up a random dataframe of size 100
np.random.seed(13)
X = np.random.normal(50, 20, size=100) 
Y = 2 * X + np.random.randint(25)
Z = np.random.choice(['pink', 'blue', 'green', 'red'], p=[0.1, 0.3, 0.4, 0.2], size=100)
df = pd.DataFrame({'X': X, 'Y': Y, 'Z': Z})
HTML(df.head().to_html(classes="table table-stripped table-hover table-dark"))
&lt;/pre&gt;

&lt;table border="1" class="dataframe table table-stripped table-hover table-dark"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;X&lt;/th&gt;
      &lt;th&gt;Y&lt;/th&gt;
      &lt;th&gt;Z&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;35.752187&lt;/td&gt;
      &lt;td&gt;85.504374&lt;/td&gt;
      &lt;td&gt;green&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;65.075328&lt;/td&gt;
      &lt;td&gt;144.150655&lt;/td&gt;
      &lt;td&gt;green&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;49.109938&lt;/td&gt;
      &lt;td&gt;112.219877&lt;/td&gt;
      &lt;td&gt;red&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;59.036247&lt;/td&gt;
      &lt;td&gt;132.072494&lt;/td&gt;
      &lt;td&gt;red&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;76.902034&lt;/td&gt;
      &lt;td&gt;167.804068&lt;/td&gt;
      &lt;td&gt;blue&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let's create a simple scatter plot of X against Y.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.scatter(X, Y)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;matplotlib.collections.PathCollection at 0x11dd45748&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="images/Matplotlib_hacks_8_1.png"&gt;&lt;/p&gt;
&lt;p&gt;Firstly, we can disable the text output of Matplotlib by placing a &lt;code&gt;;&lt;/code&gt; at the end of the code. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.scatter(X, Y);
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="png" src="images/Matplotlib_hacks_10_0.png"&gt;&lt;/p&gt;
&lt;p&gt;The resolution of this chart is not the most optimal. We can improve it by running the following Ipython magic command, which improves the definition of image outputs. I usually do this at the beginning of my notebook, right below my imports so all my charts look nice and sharp. &lt;code&gt;%config InlineBackend.figure_format = 'retina'&lt;/code&gt; works as well.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
%config InlineBackend.figure_format = 'svg'
plt.scatter(X, Y);
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_12_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;See? Much better! But, I am not sure if I am a fan of the bland white background. Let's put some grid lines in there.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.scatter(X, Y);
plt.grid(color='gray', linewidth=0.4)
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_14_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can get more granular with our grid lines if we want to by turning on minor ticks, and then customizing the minor grid by setting the &lt;code&gt;which&lt;/code&gt; argument to &lt;code&gt;minor&lt;/code&gt; inside &lt;code&gt;plt.grid()&lt;/code&gt;. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.scatter(X, Y);
plt.grid(color='gray', linewidth=0.4)
plt.minorticks_on()
plt.grid(color='lightgray', linestyle=':', linewidth=0.2, which='minor')
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_16_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;You may also notice that it's hard to separate more crowded data points from sparse data points. The &lt;code&gt;alpha&lt;/code&gt; argument allows us to adjust the transparency of each point so overlapping ones can be distinguished. An &lt;code&gt;alpha&lt;/code&gt; of 1/2 means an opaque point will appear only if there is at least 2 points that fall into that region. Decreasing the alpha will increase the threshold for opaque points.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.scatter(X, Y, alpha=1/2);
plt.grid(color='gray', linewidth=0.4)
plt.minorticks_on()
plt.grid(color='lightgray', linestyle=':', linewidth=0.2, which='minor')
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_18_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Next, let's spin up a bar chart to demonstrate how changing the default colors can help us achieve a more visually appealing aesthetic. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
# group df by each color, get sum of the X column
s = df.groupby('Z')[['X']].sum()
plt.bar(s.index, s['X'], color=['blue','green','pink','red']);
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_21_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Personally, I am not thrilled with these colors. Thankfully, Matplotlib has a wide range of color options we can use. The full list can be viewed &lt;a href="https://matplotlib.org/gallery/color/named_colors.html"&gt;here&lt;/a&gt;. I also usually like to add an edgecolor to my charts make them look a bit neater.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.bar(s.index, s['X'], color=['deepskyblue','yellowgreen','hotpink','tomato'], edgecolor='black');
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_23_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Matplotlib also has alternative styling options we can use if we want to change the look of the figure. Here is a list of all the available styles. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.style.available
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[&amp;#39;seaborn-dark&amp;#39;,
 &amp;#39;seaborn-darkgrid&amp;#39;,
 &amp;#39;seaborn-ticks&amp;#39;,
 &amp;#39;fivethirtyeight&amp;#39;,
 &amp;#39;seaborn-whitegrid&amp;#39;,
 &amp;#39;classic&amp;#39;,
 &amp;#39;_classic_test&amp;#39;,
 &amp;#39;fast&amp;#39;,
 &amp;#39;seaborn-talk&amp;#39;,
 &amp;#39;seaborn-dark-palette&amp;#39;,
 &amp;#39;seaborn-bright&amp;#39;,
 &amp;#39;seaborn-pastel&amp;#39;,
 &amp;#39;grayscale&amp;#39;,
 &amp;#39;seaborn-notebook&amp;#39;,
 &amp;#39;ggplot&amp;#39;,
 &amp;#39;seaborn-colorblind&amp;#39;,
 &amp;#39;seaborn-muted&amp;#39;,
 &amp;#39;seaborn&amp;#39;,
 &amp;#39;Solarize_Light2&amp;#39;,
 &amp;#39;seaborn-paper&amp;#39;,
 &amp;#39;bmh&amp;#39;,
 &amp;#39;tableau-colorblind10&amp;#39;,
 &amp;#39;seaborn-white&amp;#39;,
 &amp;#39;dark_background&amp;#39;,
 &amp;#39;seaborn-poster&amp;#39;,
 &amp;#39;seaborn-deep&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;pre class="prettyprint"&gt;
# run this code to set a style
plt.style.use('fivethirtyeight') 
&lt;/pre&gt;

&lt;pre class="prettyprint"&gt;
plt.figure(figsize= (4, 4))
plt.bar(s.index, s['X'], color=['deepskyblue','yellowgreen','hotpink','tomato'], edgecolor='black');
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_27_0.svg"&gt;&lt;/p&gt;
&lt;h3&gt;Figures and axes&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;You'll often see people use matplotlib in one of the following two ways: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Call plotting functions directly on the pyplot object like &lt;code&gt;plt.plot()&lt;/code&gt;, &lt;code&gt;plt.scatter()&lt;/code&gt;, &lt;code&gt;plt.bar()&lt;/code&gt;, etc&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;fig, ax = plt.subplots()&lt;/code&gt; first, and then call plotting functions on ax like &lt;code&gt;ax.plot()&lt;/code&gt;, &lt;code&gt;ax.bar()&lt;/code&gt;, etc&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What's the difference between them? &lt;/p&gt;
&lt;p&gt;TL;DR not that much, most of the time&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plt.subplots()&lt;/code&gt; is a function that returns a tuple containg a figure and an axes object. &lt;code&gt;fig, ax = plt.subplots()&lt;/code&gt; unpacks this tuple onto the fig and ax variables. Fig stores the entire figure, and ax stores the axes object created.&lt;/p&gt;
&lt;p&gt;By running &lt;code&gt;fig, ax = plt.subplots()&lt;/code&gt; you are essentially creating a matplotlib object for your plot. With &lt;code&gt;plt.plot()&lt;/code&gt;, you are still able to return a chart, but you are not creating an object. Having your chart defined as an object is useful, for example, if you want to save it as png (in which case you would call &lt;code&gt;fig.savefig('filename.png')&lt;/code&gt;) or if you are using a for loop to create multiple subplots. It can also be useful when you need access to more intricate functionalities, because creating a matplotlib object unlocks additional matplotlib features. But, for the most part, &lt;code&gt;plt.plot()&lt;/code&gt; will get done what you need to get done. &lt;/p&gt;
&lt;h3&gt;Subplots&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;With subplots, we can place multiple charts on the same figure for ease of viewing. It's especially useful when we are faceting charts by a categorical variable. Here is an example. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
# look at X vs Y, faceted by the Z column

 # let's try a different style
plt.style.use('ggplot')
fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(9.5, 3))
colors=['yellowgreen','tomato','deepskyblue','hotpink']

# example of the for loop i talked about earlier
for i, color in enumerate(df.Z.unique()): 
    filtered_df = df[df.Z == color]
    ax[i].scatter(filtered_df.X, filtered_df.Y, c=colors[i])
    ax[i].set(title=color)
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_34_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;What's happening here? I created my subplot in the first line. I set it to have 1 row and 4 columns, and I specified that each subplot should be 9.5 inches wide and 3 inches tall. Then, I created a loop that goes through all the unique elements in Z, filters the dataframe by each element, and creates a scatterplot of the filtered dataframe's X's vs Y's. &lt;/p&gt;
&lt;p&gt;One thing I don't like about this figure is that the X and Y ticks don't match, so it's hard to visually spot the differences in the spread of different colors. Normally, we would want such information to be immediately noticeable for our readers. I'll specify xlim and ylim parameters inside the set method so all the subplots have the same X and Y ticks.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
fig, ax = plt.subplots(1, 4, figsize=(9.5, 3))
colors=['yellowgreen','tomato','deepskyblue','hotpink']

for i, color in enumerate(df.Z.unique()):
    filtered_df = df[df.Z == color]
    ax[i].set(title=color, xlim=(X.min()-10, X.max()+10), ylim=(Y.min()-10, Y.max()+10)) 
    ax[i].scatter(filtered_df.X, filtered_df.Y, c=colors[i])
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_36_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;The difference is subtle but important. See how different the pink subplot looks now, and how easier it is to see that it has a much tighter range than all the other colors.&lt;/p&gt;
&lt;h3&gt;Customization&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;We have created pretty charts and subplots but we haven't yet customized our plots with titles, X and Y labels, fonts, markers etc. Matplotlib has a &lt;em&gt;toooon&lt;/em&gt; of customization options, and that's actually my favorite aspect about it (and also why I keep going back to it although it's kind of a hassle to use). Here, I'll show you a few that I most frequently use.&lt;/p&gt;
&lt;p&gt;To demonstrate this I am going to create a new column A and populate it based on the color in Z. My goal is to create a scatter plot of X vs A, in which the points will fall into clear cut groups based on what color they are. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
for size, color in zip([250, 80, 175, 30],['blue','green','pink','red']):    
    color_index = df[df.Z == color].index
    color_array = np.random.normal(size, 10, size = len(color_index))
    for i in range(len(color_index)):
        df.at[color_index[i], 'A'] = color_array[i]
&lt;/pre&gt;

&lt;p&gt;Before going further, I need to reset the style to matplotlib's default because alternative styles can override the customizations we set.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
# reset style to default matplotlib
import matplotlib as mpl
plt.rcParams.update(mpl.rcParamsDefault)

plt.figure(figsize= (5, 3))

plt.scatter(df[df['Z'] == 'green'].X, df[df['Z'] == 'green'].A
            , color='yellowgreen');

plt.scatter(df[df['Z'] == 'blue'].X, df[df['Z'] == 'blue'].A
            , color='deepskyblue');

plt.scatter(df[df['Z'] == 'pink'].X, df[df['Z'] == 'pink'].A
            , color='hotpink');

plt.scatter(df[df['Z'] == 'red'].X, df[df['Z'] == 'red'].A
            , color='tomato');
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_42_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Now let's name our plot and axes so our audience can understand what this chart is displaying.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.figure(figsize= (5, 3))

plt.scatter(df[df['Z'] == 'green'].X, df[df['Z'] == 'green'].A
            , color='yellowgreen');

plt.scatter(df[df['Z'] == 'blue'].X, df[df['Z'] == 'blue'].A
            , color='deepskyblue');

plt.scatter(df[df['Z'] == 'pink'].X, df[df['Z'] == 'pink'].A
            , color='hotpink');

plt.scatter(df[df['Z'] == 'red'].X, df[df['Z'] == 'red'].A
            , color='tomato');

plt.title('X vs A based on color group');
plt.xlabel('X values');
plt.ylabel('A values');
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_44_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Not sure I like the font, size and location of the title and labels. I can play around with these by changing the fontname, fontsize and loc arguments. I could also change the color if I wanted to with the color argument, but I like black for titles and labels.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.figure(figsize= (5, 3))

plt.scatter(df[df['Z'] == 'green'].X, df[df['Z'] == 'green'].A
            , color='yellowgreen');

plt.scatter(df[df['Z'] == 'blue'].X, df[df['Z'] == 'blue'].A
            , color='deepskyblue');

plt.scatter(df[df['Z'] == 'pink'].X, df[df['Z'] == 'pink'].A
            , color='hotpink');

plt.scatter(df[df['Z'] == 'red'].X, df[df['Z'] == 'red'].A
            , color='tomato');

plt.title('X vs A based on color group', fontname='serif'
          , fontsize=15, loc='left');
plt.xlabel('X values', fontname='serif', fontsize=11);
plt.ylabel('A values', fontname='serif', fontsize=11);
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_46_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Finally, I want to change the markers of the scatter plots for each color and put an appropriate legend.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.figure(figsize= (5, 3))

plt.scatter(df[df['Z'] == 'green'].X, df[df['Z'] == 'green'].A
            , color='yellowgreen', marker='*', s=30, label='green');

plt.scatter(df[df['Z'] == 'blue'].X, df[df['Z'] == 'blue'].A
            , color='deepskyblue', marker='v', s=30, label='blue');

plt.scatter(df[df['Z'] == 'pink'].X, df[df['Z'] == 'pink'].A
            , color='hotpink', marker='s', s=30, label='pink');

plt.scatter(df[df['Z'] == 'red'].X, df[df['Z'] == 'red'].A
            , color='tomato', marker='&lt;', s=30, label='red');

plt.legend(facecolor='lightgrey', edgecolor='black', fontsize=10);
plt.grid(color='lightgrey', linewidth=0.2); 
plt.title('X vs A based on color group', fontname='serif'
          , fontsize=15, loc='left');
plt.xlabel('X values', fontname='serif', fontsize=11);
plt.ylabel('A values', fontname='serif', fontsize=11);
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_48_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;Note that if we had defined matplotlib objects to create the charts, we would use the following methods on our axes objects in place of the ones we used:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;plt.title()&lt;/code&gt; --&amp;gt; &lt;code&gt;ax.set_title()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;plt.xlabel()&lt;/code&gt; --&amp;gt; &lt;code&gt;ax.set_xlabel()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;plt.ylabel()&lt;/code&gt; --&amp;gt; &lt;code&gt;ax.set_ylabel()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Some extra stuff&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;To change the size of a chart you have to run &lt;code&gt;plt.figure(figsize=(width, height))&lt;/code&gt; before you run your plotting function. Otherwise, the size of your chart won't actually change. This was something that I mixed up a lot in the past. This is &lt;em&gt;unless&lt;/em&gt; you are defining matplotlib objects with &lt;code&gt;fig, ax = plt.subplots()&lt;/code&gt;. In that case, you set your figsize inside the &lt;code&gt;subplots()&lt;/code&gt; method. &lt;/p&gt;
&lt;p&gt;I'm going to demonstrate this with a time series chart, so I'm adding a column of random dates to the dataframe.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;
df['date'] = pd.date_range('20180101','20181230')[0:100]

# set new style
plt.style.use('bmh')  

# ignore this part, running it to disable a depreciation warning
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

plt.plot(df['date'], df['X']);
plt.figure(figsize= (10, 2.5));
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_54_0.svg"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;Figure size 1200x500 with 0 Axes&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Yikes. As can be seen, the size of the plot was not set to 10 inches wide and 2.5 inches tall when we ran &lt;code&gt;plt.figure()&lt;/code&gt; after &lt;code&gt;plt.plot()&lt;/code&gt;. Instead of changing figure size, the &lt;code&gt;plt.figure()&lt;/code&gt; function returned the text output we see below the chart. In other bad news, the xtick labels are overlapping and the frequency with which matplotlib has decided to show the dates doesn't make sense. Let's fix these. It would be nice, for example, if we had x tick labels for the Monday of every week. &lt;/p&gt;
&lt;pre class="prettyprint"&gt;
plt.figure(figsize= (10, 2.5))
plt.plot(df['date'], df['X']);

# rotate xticks by a 45 degree angle so they're more legible
plt.xticks(rotation=45, fontsize=9) 
plt.yticks(fontsize=9)

# module we need to import so matplotlib can identify days of week from datetime 
import matplotlib.dates as mdates 

# get current axis
ax = plt.gca() 
ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=0, interval=1)) # tick on mondays every week
&lt;/pre&gt;

&lt;p&gt;&lt;img alt="svg" src="images/Matplotlib_hacks_56_0.svg"&gt;&lt;/p&gt;
&lt;p&gt;What happened here? I had to import a submodule called mdates from the dates module of Matplotlib to achieve the weekly X tick appearance I was aiming for. Since the xaxis method only exists for axes objects, I had to get the current axis of my plot with the &lt;code&gt;.gca()&lt;/code&gt; method.  This is an example for when you may actually need to define matplotlib objects!&lt;/p&gt;
&lt;p&gt;Then, by passing the weekday locator inside the set_major_locator method, I grabbed the days that were Mondays from the date column of my dataframe and set them as my X tick labels.&lt;/p&gt;
&lt;p&gt;To be honest, I didn't know how to this. I just googled something along the lines of "matplotlib set custom X ticks for dates" and was able to find an answer within max 10-15 minutes. &lt;em&gt;That's precisely the beauty of Matplotlib&lt;/em&gt;; it's been around for so long that there is an answer for pretty much any problem you may encounter! &lt;/p&gt;
&lt;p&gt;... aand that's a wrap! I wish you all the very best in your data viz journey with Matplotlib!&lt;/p&gt;</content><category term="python"></category></entry></feed>